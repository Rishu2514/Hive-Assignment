Scenario Based questions:

Q-1-Will the reducer work or not if you use “Limit 1” in any HiveQL query?
A-1-No,reducer will not work in this case as the reducer is responsible for aggregating and reducing the data after the map phase in a
MapReduce job.The reducer's task is to perform operations like grouping, sorting, and reducing the data before producing the final
output.


Q-2-Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. Then, what will happen 
if we have multiple clients trying to access Hive at the same time? 
A-2-The default metastore configuration in Hive uses a relational database, such as MySQL or Derby, to store metadata about tables, 
partitions, and other Hive objects. When multiple clients try to access Hive simultaneously, they will be interacting with the 
metastore concurrently.Hive's default metastore configuration allows for concurrent access and handles concurrency through the 
underlying database's concurrency control mechanisms. These mechanisms ensure data consistency and integrity when multiple clients 
access and modify the metadata concurrently.the concurrency control mechanisms in the underlying database, such as locks, transactions
and isolation levels, handle concurrent access to the metastore. These mechanisms ensure that multiple clients can read and write
metadata in a controlled and consistent manner.However, it's worth noting that if there is heavy concurrent access to the metastore,
it can potentially create contention and impact performance. In such cases, it might be necessary to consider tuning the metastore or 
exploring alternative metastore solutions that offer better scalability for concurrent access.


Q-3-Suppose, I create a table that contains details of all the transactions done by the customers: CREATE TABLE transaction_details 
(cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;Now, after inserting 50,000
records in this table, I want to know the total revenue generated for each month. But, Hive is taking too much time in processing this
query. How will you solve this problem and list the steps that I will be taking in order to do so?
A-3-As we know, we can’t partition an existing non-partitioned table directly. So, we will be taking following steps to solve the very
problem: 
(1). Create a partitioned table, say partitioned_transaction:
CREATE TABLE partitioned_transaction (cust_id INT, amount FLOAT, country STRING) PARTITIONED BY (month STRING) ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ‘,’ ; 
(2). Enable dynamic partitioning in Hive:
SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;
(3). Transfer the data from the non – partitioned table into the newly created partitioned table:
INSERT OVERWRITE TABLE partitioned_transaction PARTITION (month) SELECT cust_id, amount, country, month FROM transaction_details;
Now, we can perform the query using each partition and therefore, decrease the query time.


Q-4-How can you add a new partition for the month December in the above partitioned table?
A-4-For adding a new partition in the above table partitioned_transaction, we will issue the command give below:
ALTER TABLE partitioned_transaction ADD PARTITION (month=’Dec’) LOCATION  ‘/partitioned_transaction’;


Q-5-I am inserting data into a table based on partitions dynamically. But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: 
Dynamic partition strict mode requires at least one static partition column. How will you remove this error?
A-5-To remove this error one has to execute following commands:
SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;




Q-6-Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries:
id first_name last_name email gender ip_address
How will you consume this CSV file into the Hive warehouse using built-in SerDe?
A-6-SerDe stands for serializer/deserializer. A SerDe allows us to convert the unstructured bytes into a record that we can process
using Hive. SerDes are implemented using Java. Hive comes with several built-in SerDes and many other third-party SerDes are also
available. 
Hive provides a specific SerDe for working with CSV files. We can use this SerDe for the sample.csv by issuing following commands:
CREATE EXTERNAL TABLE sample
(
id int, first_name string, 
last_name string, email string,
gender string, ip_address string
) 
ROW FORMAT SERDE ‘org.apache.hadoop.hive.serde2.OpenCSVSerde’ 
STORED AS TEXTFILE LOCATION ‘/temp’;
Now, we can perform any query on the table ‘sample’:
SELECT first_name FROM sample WHERE gender = ‘male’;


Q-7-Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table 
corresponding to these files. The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop 
performance degrades when we use lots of small files.So, how will you solve this problem where we want to create a single Hive table 
for lots of small files without degrading the performance of the system?
A-7-One can use the SequenceFile format which will group these small files together to form a single sequence file. The steps that 
will be followed in doing so are as follows:
Create a temporary table:
CREATE TABLE temp_table (id INT, name STRING, e-mail STRING, country STRING)
ROW FORMAT FIELDS DELIMITED TERMINATED BY ‘,’ STORED AS TEXTFILE;
Load the data into temp_table:
LOAD DATA INPATH ‘/input’ INTO TABLE temp_table;
Create a table that will store data in SequenceFile format:
CREATE TABLE sample_seqfile (id INT, name STRING, e-mail STRING, country STRING)
ROW FORMAT FIELDS DELIMITED TERMINATED BY ‘,’ STORED AS SEQUENCEFILE;
Transfer the data from the temporary table into the sample_seqfile table:
INSERT OVERWRITE TABLE sample SELECT * FROM temp_table;
Hence, a single SequenceFile is generated which contains the data present in all of the input files and therefore, the problem of 
having lots of small files is finally eliminated.



Q-8-LOAD DATA LOCAL INPATH ‘Home/country/state/’
    OVERWRITE INTO TABLE address;
The following statement failed to execute. What can be the cause?
Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?
A-8-There can be several possible causes for the failure:
*Incorrect file path: The file path specified in the INPATH clause may be incorrect.
*File permissions: The user running the Hive query may not have the necessary permissions to access or load the data from the 
specified file path. Check the file permissions to ensure that the user has appropriate read access to the file.
*Table or column mismatch: The destination table address may not exist or may have a different schema than the data being loaded. 
Ensure that the table address exists and has the correct structure to match the data being loaded.

In a Hive cluster, the number of nodes is determined by the underlying Hadoop cluster. Hive relies on Hadoop's distributed processing
framework, and the number of nodes in a Hive cluster corresponds to the number of nodes in the Hadoop cluster.If you already have 100 
nodes in your existing Hadoop cluster, it is possible to add 100 more nodes to increase the overall cluster size. To achieve this, you
would need to expand the underlying Hadoop cluster and not specifically the Hive cluster.
Expanding a Hadoop cluster typically involves the following steps:
*Set up new machines: Prepare the new machines that will become part of the cluster. Install the necessary software, configure network
settings, and ensure they meet the hardware requirements.
*Install and configure Hadoop: Install and configure Hadoop on the new machines, following the same steps you followed for the initial
cluster setup. This includes setting up HDFS (Hadoop Distributed File System) and YARN (Yet Another Resource Negotiator) services.
*Update Hadoop cluster configuration: Update the configuration files of the existing Hadoop cluster to include the information about 
the new nodes. This involves adding the IP addresses or hostnames of the new nodes to the relevant configuration files
*Start the new nodes: Start the Hadoop services on the new machines to make them part of the cluster. This typically involves starting
services such as DataNodes and NodeManagers, which handle storage and processing on the new nodes, respectively
*Verify the cluster: Once the new nodes are added and services are running, verify the cluster status using Hadoop cluster management
tools, such as the Hadoop cluster web UI or command-line utilities.


Hive Practical questions:

Q-9-Hive Join operations
Create a  table named CUSTOMERS(ID | NAME | AGE | ADDRESS   | SALARY)
Create a Second  table ORDER(OID | DATE | CUSTOMER_ID | AMOUNT)
Now perform different joins operations on top of these tables
(Inner JOIN, LEFT OUTER JOIN ,RIGHT OUTER JOIN ,FULL OUTER JOIN)
A-9-
create database hive_db;
use hive_db;
create table customers
(
id int,
name string,
age int,
address string,
salary int
)
row format delimited
fields terminated by ',';
load data local inpath 'file:///config/workspace/customers.csv' into table customers;
create table order
(
oid int,
date date,
customer_id int,
amount int
)
row format delimited
fields terminated by ',';
load data local inpath 'file:///config/workspace/order.csv' into table order;
create table buck_customers
(
id int,
name string,
age int,
address string,
salary int
)
clustered by (id)
sorted by (id)
into 2 buckets;
insert overwrite table buck_customers select * from customers;
create table buck_order
(
oid int,
date date,
customer_id int,
amount int
)
clustered by (id)
sorted by (id)
into 2 buckets;
insert overwrite table buck_order select * from order;
***Reduce-Side Join****
SET hive.auto.convert.join=false;
SELECT * FROM buck_customers c INNER JOIN buck_order o ON c.id = o.customer_id;
****Map Side Join*****
SET hive.auto.convert.join=true;
SELECT * FROM buck_customers c INNER JOIN buck_order o ON c.id = o.customer_id;
*****Bucket Map Join******
set hive.optimize.bucketmapjoin=true;
SET hive.auto.convert.join=true;
SELECT * FROM buck_customers c INNER JOIN buck_order o ON c.id = o.customer_id;
********Sorted Merge Bucket Map Join*********
set hive.enforce.sortmergebucketmapjoin=false;
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
SELECT * FROM buck_customers c INNER JOIN buck_order o ON c.id = o.customer_id;


BUILD A DATA PIPELINE WITH HIVE

Q-10-Download a data from the given location - 
https://archive.ics.uci.edu/ml/machine-learning-databases/00360/
1. Create a hive table as per given schema in your dataset 
2. try to place a data into table location
3. Perform a select operation . 
4. Fetch the result of the select operation in your local as a csv file . 
5. Perform group by operation . 
7. Perform filter operation at least 5 kinds of filter examples . 
8. show and example of regex operation
9. alter table operation 
10 . drop table operation
12 . order by operation . 
13 . where clause operations you have to perform . 
14 . sorting operation you have to perform . 
15 . distinct operation you have to perform . 
16 . like an operation you have to perform . 
17 . union operation you have to perform . 
18 . table view operation you have to perform . 
A-10-
create table AirQualityUCI
(
date string,
time string,
co string,
pt string,
nmhc int,
c6h6 string,
pt08_s2 int,
nox int,
pt08_s3 int,
no2 int,
pt08_s4 int,
pt08_s5 int,
t string,
rh string,
ah string,
blanck_col string
)
row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde' 
with serdeproperties ( 
"separatorChar" = "\073" 
)
stored as textfile
tblproperties ("skip.header.line.count" = "1"); 





Q-11-hive operation with python

Create a python application that connects to the Hive database for extracting data, creating sub tables for data processing, drops 
temporary tables.fetch rows to python itself into a list of tuples and mimic the join or filter operations

A-11-
import pyhive
from pyhive import hive

# Connect to Hive
conn = hive.Connection(host='localhost', port=10000, username='your_username')
cursor = conn.cursor()

# Execute Hive queries
cursor.execute('USE your_database')

# Extract data
cursor.execute('SELECT * FROM your_table')
data = cursor.fetchall()

# Close the connection
cursor.close()
conn.close()

# Process data
# ...

# Create sub-tables
# ...

# Drop temporary tables
# ...

# Fetch rows into a list of tuples
result = []
for row in data:
    result.append(tuple(row))

# Perform join or filter operations
# ...

# Example join operation
# Assuming two tables: table1 and table2 with a common column 'id'
join_query = '''
SELECT *
FROM table1
JOIN table2
  ON table1.id = table2.id
'''
cursor = conn.cursor()
cursor.execute(join_query)
join_result = cursor.fetchall()

# Close the connection
cursor.close()
conn.close()

# Example filter operation
# Assuming a table named 'data' with a column 'value' to filter on
filter_query = '''
SELECT *
FROM data
WHERE value > 10
'''
cursor = conn.cursor()
cursor.execute(filter_query)
filter_result = cursor.fetchall()

# Close the connection
cursor.close()
conn.close()





















